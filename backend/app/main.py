import os
import time
import hashlib
import re
import pickle
import networkx as nx
from pathlib import Path
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer, CrossEncoder
import litellm

# Ensure you have app/security.py with the scan_snippet function
from app.security import scan_snippet 

# --- CONFIGURATION ---
INDEX_NAME = 'infrasyntax'
RETRIEVAL_MODEL = 'all-mpnet-base-v2'               # Fast Bi-Encoder
RERANK_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2' # Accurate Cross-Encoder
ELASTICSEARCH_URL = "http://127.0.0.1:9200"

# Path to the dependency graph file generated by scripts/graph_builder.py
GRAPH_FILE = Path(__file__).parent.parent.parent / "data" / "dependency_graph.pkl"

# API Keys for Explanations
litellm.suppress_instrumentation = True 

app_state = {}

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("Loading Retrieval Model (Bi-Encoder)...")
    app_state['retrieval_model'] = SentenceTransformer(RETRIEVAL_MODEL)
    
    print("Loading Re-ranking Model (Cross-Encoder)...")
    app_state['reranker'] = CrossEncoder(RERANK_MODEL)
    
    # --- LOAD KNOWLEDGE GRAPH ---
    if GRAPH_FILE.exists():
        print(f"Loading Knowledge Graph from {GRAPH_FILE}...")
        try:
            with open(GRAPH_FILE, 'rb') as f:
                app_state['graph'] = pickle.load(f)
            print(f"--- Graph loaded: {app_state['graph'].number_of_nodes()} nodes ---")
        except Exception as e:
            print(f"Failed to load graph: {e}")
            app_state['graph'] = nx.DiGraph()
    else:
        print("⚠️ Warning: No Knowledge Graph found. Run scripts/graph_builder.py first.")
        app_state['graph'] = nx.DiGraph() # Fallback to empty graph
    
    print(f"Connecting to Elasticsearch at {ELASTICSEARCH_URL}...")
    es_client = None
    while True:
        try:
            es_client = Elasticsearch(
                [ELASTICSEARCH_URL], 
                verify_certs=False, 
                ssl_show_warn=False
            )
            if es_client.ping():
                print("--- Elasticsearch connection successful! ---")
                break
            else:
                print("Ping failed. Retrying in 5s...")
                time.sleep(5)
        except Exception:
            time.sleep(5)
    
    app_state['es'] = es_client
    yield
    app_state.clear()

app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- DATA MODELS ---
class SearchRequest(BaseModel):
    query: str
    num_results: int = 10

class AuditRequest(BaseModel):
    code: str
    technology: str

class ExplainRequest(BaseModel):
    query: str
    code: str

# --- HELPER FUNCTIONS ---

def normalize_code(code: str) -> str:
    """Removes whitespace/comments to create a fingerprint for deduplication."""
    code = re.sub(r'#.*', '', code)
    code = re.sub(r'//.*', '', code)
    return ''.join(code.split())

def deduplicate_results(hits):
    """Filters out duplicate code snippets."""
    seen_hashes = set()
    unique_hits = []
    
    for hit in hits:
        code_content = hit['_source']['code']
        code_hash = hashlib.md5(normalize_code(code_content).encode('utf-8')).hexdigest()
        
        if code_hash not in seen_hashes:
            seen_hashes.add(code_hash)
            unique_hits.append(hit)
            
    return unique_hits

def rerank_results(reranker, query, initial_results):
    """Uses Cross-Encoder to re-score results for high precision."""
    if not initial_results:
        return []
    
    pairs = [[query, hit['_source']['description']] for hit in initial_results]
    scores = reranker.predict(pairs)
    
    for i, hit in enumerate(initial_results):
        hit['_score'] = scores[i]
        
    return sorted(initial_results, key=lambda x: x['_score'], reverse=True)

# --- ENDPOINTS ---

@app.post("/search")
async def search_snippets(request: SearchRequest):
    es = app_state['es']
    retrieval_model = app_state['retrieval_model']
    reranker = app_state['reranker']
    graph = app_state['graph']

    # 1. RETRIEVAL
    query_vector = retrieval_model.encode(request.query).tolist()
    
    search_query = {
        "size": 100, 
        "query": {
            "script_score": {
                "query": {
                    "bool": {
                        "should": [
                            {"multi_match": {"query": request.query, "fields": ["description^3", "code"]}}
                        ]
                    }
                },
                "script": {
                    "source": "cosineSimilarity(params.query_vector, 'snippet_vector') + 1.0",
                    "params": {"query_vector": query_vector}
                }
            }
        },
        "_source": ["id", "description", "code", "technology", "source_file"] 
    }
    
    response = es.search(index=INDEX_NAME, body=search_query)
    raw_hits = response['hits']['hits']

    # 2. DEDUPLICATION
    unique_hits = deduplicate_results(raw_hits)

    # 3. RE-RANKING
    candidates_to_rank = unique_hits[:50]
    ranked_hits = rerank_results(reranker, request.query, candidates_to_rank)
    initial_top_results = ranked_hits[:request.num_results]
    
    # 4. GRAPH AUGMENTATION (GraphRAG)
    enhanced_results = []
    seen_ids = set()
    
    # Pre-fill seen_ids with the initial results to avoid duplicates
    for hit in initial_top_results:
        # Robust ID extraction: Try 'id' field, fallback to ES '_id'
        sid = hit['_source'].get('id', hit['_id'])
        seen_ids.add(sid)

    for hit in initial_top_results:
        source_data = hit['_source']
        source_data['is_dependency'] = False 
        enhanced_results.append(source_data)
        
        current_id = source_data.get('id', hit['_id'])
        
        # Check Graph for neighbors
        if current_id in graph:
            # Get outgoing edges (dependencies)
            neighbors = list(graph.successors(current_id))
            
            for neighbor_id in neighbors:
                if neighbor_id not in seen_ids:
                    try:
                        # Fetch dependency details from ES
                        # Note: This assumes neighbor_id matches the ES _id or we query for it
                        # If you stored custom IDs in graph, you might need a term query here.
                        # Assuming direct ID lookup for performance:
                        dep_hit = es.get(index=INDEX_NAME, id=neighbor_id)
                        dependency = dep_hit['_source']
                        
                        dependency['is_dependency'] = True 
                        dependency['related_to'] = source_data.get('description', 'Parent Snippet')
                        
                        enhanced_results.append(dependency)
                        seen_ids.add(neighbor_id)
                    except Exception:
                        # Dependency might exist in graph but not in index
                        pass
    
    return {"results": enhanced_results}

@app.post("/audit")
async def audit_snippet(request: AuditRequest):
    """Scans snippet for security vulnerabilities."""
    return scan_snippet(request.code, request.technology)

@app.post("/explain")
async def explain_snippet(request: ExplainRequest):
    """Generates natural language explanation via LiteLLM."""
    try:
        model_name = "gpt-3.5-turbo" 
        prompt = f"""
        User Query: "{request.query}"
        Code Snippet: ```{request.code}```
        Explain concisely (in 2 sentences) how this code achieves the user's goal.
        """
        response = await litellm.acompletion(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100
        )
        return {"explanation": response.choices[0].message.content}
    except Exception as e:
        print(f"Explanation failed: {e}")
        return {"explanation": "Could not generate explanation at this time."}